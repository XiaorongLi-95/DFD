# DFD
PyTorch implementation of our paper "Memory Efficient Data-Free Distillation for Continual Learning", published in Pattern Recognition.

**Title**: [Memory Efficient Data-Free Distillation for Continual Learning]()

**Authors**: Xiaorong Li, Shipeng Wang, Jian Sun, Zongben Xu

**Email:** lixiaorong@stu.xjtu.edu.cn, lixiaorongxjtu@gmail.com

## Usage

```
sh scripts/dfd.sh
```

#### Prepare Dataset

For 20mini, you can download this dataset at [here](https://drive.google.com/file/d/1gPxfd4B6pnHnaV8LEFQDqo-6JMA7HQZL/view?usp=sharing).

## Requirements

Python (3.6) 

PyTorch (1.9.0) 

## Citation

```
@ARTICLE{Li_2023_pr,
  author={Xiaorong Li and Shipeng Wang and Jian Sun and Zongben Xu},
  journal={Pattern Recognition},
  volumn={144},
  pages={109875},
  year={2023},
  issn={0031-3203}
```

## Acknowledgment

The code is based on [Adam-NSCL](https://github.com/ShipengWang/Adam-NSCL) and [Continual-Learning-Benchmark](https://github.com/GT-RIPL/Continual-Learning-Benchmark).
